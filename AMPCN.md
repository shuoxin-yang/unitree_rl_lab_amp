# AMP：用于风格化基于物理角色控制的对抗性运动先验

薛斌鹏∗，加州大学伯克利分校，美国；马泽∗，上海交通大学，中国；彼得・阿贝尔，加州大学伯克利分校，美国；谢尔盖・莱文，加州大学伯克利分校，美国；安珠・金泽，加州大学伯克利分校，美国

图 1. 我们的框架能够让物理模拟角色在完成具有挑战性的任务时，采用非结构化运动数据所指定的风格化行为。左图：角色学习运用多种移动技能穿越障碍赛道；右图：角色学习走向目标并对其出拳。

为基于物理模拟的角色合成优雅且逼真的行为，一直是计算机动画领域的核心挑战。利用运动跟踪的数据驱动方法是一类重要技术，可针对多种行为生成高保真度运动。然而，这类基于跟踪的方法效果往往依赖精心设计的目标函数，且应用于大型多样化运动数据集时，需要额外的复杂机制来为角色选择在特定场景下应跟踪的合适运动。在本文中，我们提出一种基于对抗性模仿学习的全自动方法，无需手动设计模仿目标和运动选择机制。角色需完成的高层任务目标可通过相对简单的奖励函数指定，而角色行为的低层风格可通过非结构化运动片段数据集指定，无需任何显式的片段选择或排序。例如，穿越障碍赛道的角色可采用仅考虑前进进度的任务奖励，而数据集包含跑步、跳跃和翻滚等相关行为片段。这些运动片段用于训练对抗性运动先验，该先验通过强化学习（RL）为角色训练提供风格奖励。对抗性强化学习过程会自动选择要执行的运动，从数据集中动态插值并泛化。我们的系统生成的高质量运动可与最先进的基于跟踪的技术相媲美，同时还能轻松适应大型非结构化运动片段数据集。不同技能的组合会从运动先验中自动涌现，无需高层运动规划器或对运动片段进行其他任务特定标注。我们在多种复杂模拟角色和一系列具有挑战性的运动控制任务上，验证了该框架的有效性。

CCS 概念：・计算方法学 → 过程动画；对抗性学习；控制方法。

关键词：角色动画、强化学习、对抗性模仿学习

ACM 参考文献格式：

薛斌鹏、马泽、彼得・阿贝尔、谢尔盖・莱文和安珠・金泽。2021。AMP：用于风格化基于物理角色控制的对抗性运动先验。《ACM 图形学 Transactions》40 卷 4 期，第 144 篇（2021 年 8 月），20 页。[https://doi.org/10.1145/3450626.3459670](https://doi.org/10.1145/3450626.3459670)

## 1 引言

为虚拟角色合成自然逼真的运动，是为电影、游戏等沉浸式体验注入生命力的关键要素。在虚拟现实（VR）应用中，用户可通过丰富的模态与虚拟智能体交互，对逼真运动的需求更为迫切。开发能够复制自然行为特性的控制策略，对机器人系统也具有重要意义，因为自然运动隐含了安全性、能量效率等重要特性，这些对机器人在现实世界中的有效运行至关重要。尽管自然运动的例子随处可见，但识别构成这些行为的潜在特征仍具挑战性，要在控制器中复现则更难。

那么，构成自然逼真行为的特征是什么？设计运动自然度的定量指标，一直是基于优化的角色动画技术面临的核心挑战 \[Al Borno 等人，2013；Wampler 等人，2014；Wang 等人，2009]。对称性、稳定性和能量最小化等启发式方法可提高基于物理模拟角色的运动逼真度 \[Grochow 等人，2004；Mordatch 等人，2012，2013；Yu 等人，2018]。但这些策略可能无法广泛适用于所有感兴趣的行为。要有效应用这些启发式方法，通常需要仔细平衡各种目标，且每个任务可能都需要重复调整过程。数据驱动方法通过利用真实演员录制的运动片段来指导模拟角色的行为，能够缓解部分此类挑战 \[Da Silva 等人，2008；Liu 等人，2010；Muico 等人，2009；Sok 等人，2007]。这类方法的常见实现方式是采用跟踪目标，鼓励角色遵循特定任务相关的参考轨迹。这些基于跟踪的方法可生成多种技能的高质量运动，但要有效利用大型非结构化运动数据集仍具挑战，因为需要在每个时间步为角色选择合适的运动片段进行跟踪。这种选择过程通常由运动规划器完成，其为解决特定任务生成参考轨迹 \[Bergamin 等人，2019；Park 等人，2019；Peng 等人，2017]。然而，构建有效的运动规划器本身就是一项艰巨的任务，且需要大量开销来标注和组织特定任务的运动片段。

在许多应用中，并非必须精确跟踪特定参考运动。由于数据集通常仅提供有限的示例运动集合，角色为有效完成特定任务，不可避免地需要偏离参考运动。因此，其目的往往不是让角色紧密跟踪特定运动，而是采用数据集中所描绘的一般行为特征。我们将这些行为特征称为风格。

在本文中，我们旨在开发一个系统，用户可指定角色需完成的高层任务目标，同时角色运动的低层风格可通过非结构化运动片段形式的示例进行控制。为控制角色运动的风格，我们提出对抗性运动先验（AMP），这是一种从原始运动片段中模仿行为的方法，无需任何任务特定标注或数据集组织。给定一组构成期望运动风格的参考运动，运动先验被建模为对抗性判别器，训练其区分数据集中描绘的行为与角色产生的行为。因此，运动先验可作为角色产生的运动与数据集中运动之间相似性的通用度量。通过将运动先验融入目标条件强化学习框架，我们的系统能够训练基于物理模拟的角色，以自然逼真的行为完成具有挑战性的任务。多种行为的组合会从运动先验中自动涌现，无需运动规划器或其他模仿片段选择机制。

本文的核心贡献是一种用于基于物理角色动画的对抗性学习方法，该方法将目标条件强化学习与对抗性运动先验相结合，能够通过示例运动片段控制角色运动的风格，同时通过简单的奖励函数指定任务。我们提出了首个能够为基于物理模拟的角色生成高质量全身运动的对抗性学习系统之一。通过将运动先验与额外的任务目标相结合，我们的系统提供了便捷的接口，用户可通过该接口指定控制角色行为的高层指令。这些任务目标使我们的角色能够获得比原始运动片段中展示的更复杂的技能。尽管我们的系统基于著名的对抗性模仿学习技术，但我们提出了多项重要设计决策，使得结果质量显著高于先前的工作，能够让角色从非结构化运动数据中学习高度动态和多样化的运动技能。

## 2 相关工作

开发能够为虚拟角色合成自然运动的系统，是计算机动画的核心挑战之一。这些过程动画技术大致可分为运动学方法和基于物理的方法。运动学方法通常不明确利用运动方程进行运动合成，而是经常利用运动片段数据集为角色生成运动 \[Lee 等人，2002，2010b]。给定运动数据集，可构建控制器为特定场景选择合适的运动片段进行回放 \[Agrawal 和 van de Panne，2016；Safonova 和 Hodgins，2007；Treuille 等人，2007]。使用生成模型（如高斯过程 \[Levine 等人，2012；Ye 和 Liu，2010] 和神经网络 \[Holden 等人，2017；Ling 等人，2020；Zhang 等人，2018]）的数据驱动方法也已应用于在线运动合成。当提供足够大且高质量的数据集时，运动学方法能够为多种复杂技能生成逼真的运动 \[Agrawal 和 van de Panne，2016；Lee 等人，2018，2010b；Levine 等人，2011；Starke 等人，2019]。然而，它们为新场景合成运动的能力可能受到数据可用性的限制。对于复杂任务和环境，可能难以收集足够的数据来涵盖角色可能需要执行的所有行为。这对于非人类和虚构生物尤其具有挑战性，因为它们的运动数据可能稀缺。在本文中，我们将数据驱动技术与基于物理的动画方法相结合，开发能够对新任务和环境产生逼真且响应性行为的角色。

### 基于物理的方法

基于物理的方法通过从基本原理合成运动，解决了运动学方法的部分局限性。这些方法通常利用物理模拟或更通用的运动方程知识，为角色生成运动 \[Raibert 和 Hodgins，1991；Wampler 等人，2014]。轨迹优化和强化学习等优化技术在许多基于物理的方法中起着关键作用，通过优化目标函数生成驱动角色运动的控制器 \[Mordatch 等人，2012；Tan 等人，2014；van de Panne 等人，1994]。尽管这些方法能够在缺乏运动数据的情况下，为新场景合成物理上合理的运动，但设计能够产生自然行为的有效目标函数可能异常困难。通常会将从自然运动特征先验知识中获得的启发式方法纳入目标函数，例如对称性、稳定性、能量最小化等 \[Mordatch 等人，2012；Wang 等人，2009；Yu 等人，2018]。模拟更符合生物特性的执行器也可提高运动质量 \[Geijtenbeek 等人，2013；Jiang 等人，2019；Wang 等人，2012]，但仍可能产生不自然的行为。

### 模仿学习

设计能够产生自然运动的目标函数所面临的挑战，推动了数据驱动的基于物理动画技术的采用 \[Da Silva 等人，2008；Kwon 和 Hodgins，2017；Lee 等人，2010a；Sharon 和 van de Panne，2005；Zordan 和 Hodgins，2002]，该技术利用参考运动数据来提高运动质量。参考运动通常通过模仿目标纳入，鼓励角色模仿数据集中的运动。模仿目标通常实现为跟踪目标，试图最小化模拟角色与参考运动目标姿态之间的姿态误差 \[Lee 等人，2010a；Liu 等人，2016，2010；Peng 等人，2018a；Sok 等人，2007]。由于姿态误差通常是针对单个目标姿态计算的，因此需要谨慎从数据集中选择合适的目标姿态。一种简单的策略是使用相位变量将模拟角色与给定参考运动同步 \[Lee 等人，2019；Peng 等人，2018a,b]，该相位变量作为额外输入提供给控制器。然后，可根据相位方便地确定每个时间步的目标姿态。该策略对于模仿单个运动片段是有效的，但难以扩展到包含多个不同运动的数据集，因为可能无法根据单个相位变量同步和对齐多个参考运动。最近的方法通过将正在跟踪的参考运动中的目标姿态明确作为输入提供给控制器，将这些基于跟踪的技术扩展到更大的运动数据集 \[Bergamin 等人，2019；Chentanez 等人，2018；Park 等人，2019；Won 等人，2020]。这使得控制器能够根据输入的目标姿态模仿不同的运动。然而，在特定场景下为角色选择合适的模仿运动，仍然需要大量的算法开销。这些方法通常需要高层运动规划器，为特定任务选择角色应模仿的运动片段 \[Bergamin 等人，2019；Park 等人，2019；Peng 等人，2017]。因此，角色在特定任务上的性能可能受到运动规划器性能的限制。

基于跟踪的模仿技术的另一个主要局限性是，在计算跟踪目标时需要姿态误差度量 \[Liu 等人，2010；Peng 等人，2018a；Sharon 和 van de Panne，2005]。这些误差度量通常是手动设计的，构建和调整一种适用于角色需模仿的所有技能的通用度量具有挑战性。对抗性模仿学习提供了一种有吸引力的替代方案 \[Abbeel 和 Ng，2004；Ho 和 Ermon，2016；Ziebart 等人，2008]，与使用手动设计的模仿目标不同，这些算法训练对抗性判别器来区分智能体生成的行为与演示数据（如参考运动）中描绘的行为。然后，判别器作为目标函数，用于训练控制策略来模仿演示。尽管这些方法在运动模仿任务中显示出良好的结果 \[Merel 等人，2017；Wang 等人，2017]，但对抗性学习算法众所周知具有不稳定性，且生成的运动质量仍远落后于最先进的基于跟踪的技术。Peng 等人 \[2019b] 通过用信息瓶颈正则化判别器，能够生成更逼真的运动。然而，他们的方法仍然需要相位变量来同步策略和判别器与参考运动。因此，其结果仅限于每个策略模仿单个运动，不适合从大型多样化运动数据集学习。在本文中，我们提出一种对抗性方法，用于从包含多样化运动片段的大型非结构化数据集中学习通用运动先验。我们的方法不需要策略和参考运动之间的任何同步。此外，我们的方法不需要运动规划器，或对运动片段进行任何任务特定标注和分割 \[Bergamin 等人，2019；Park 等人，2019；Peng 等人，2017]。相反，为实现任务目标而进行的多种运动组合，会通过运动先验自动涌现。我们还提出了多项设计决策，以稳定对抗性训练过程，从而获得一致的高质量结果。

### latent 空间模型

latent 空间模型也可作为一种运动先验形式，产生更逼真的行为。这些模型通过学习到的 latent 表示指定控制，然后将其映射到基础系统的控制 \[Burgard 等人，2008；Florensa 等人，2017；Hausman 等人，2018；Heess 等人，2016]。 latent 表示通常通过预训练阶段学习，使用监督学习或强化学习技术将多种行为编码到 latent 表示中。训练完成后，该 latent 表示可用于构建控制层次结构，其中 latent 空间模型作为低层控制器，训练单独的高层控制器通过 latent 空间指定控制 \[Florensa 等人，2017；Haarnoja 等人，2018；Lynch 等人，2020]。对于模拟角色的运动控制，可训练 latent 表示来编码参考运动片段中的行为，从而将角色的行为约束为与运动数据中观察到的行为相似，进而为下游任务产生更自然的行为 \[Merel 等人，2019；Peng 等人，2019a]。然而，由于角色运动的逼真度是通过 latent 表示隐式强制的，而非通过目标函数显式强制，高层控制器仍可能指定产生不自然行为的 latent 编码 \[Merel 等人，2020；Peng 等人，2019a]。Luo 等人 \[2020] 提出了对抗性领域混淆损失，以防止高层控制器指定与预训练期间观察到的编码不同的编码。然而，由于这种对抗性目标应用于 latent 空间，而非角色产生的实际运动，该模型仍然容易生成不自然的行为。我们提出的运动先验直接强制角色产生的运动与参考运动数据集中的运动之间的相似性，这使得我们的方法能够生成比 latent 空间模型所展示的更高保真度的运动。我们的运动先验也不需要单独的预训练阶段，而是可以与策略联合训练。

## 3 概述

给定参考运动数据集和由奖励函数定义的任务目标，我们的系统合成一种控制策略，使角色能够在基于物理的模拟环境中实现任务目标，同时采用与数据集中运动相似的行为。关键的是，角色的行为不必与数据集中的任何特定运动完全匹配，其运动只需采用参考运动集合所展现的更一般特征。这些参考运动共同提供了基于示例的行为风格定义，通过为系统提供不同的运动数据集，可训练角色以多种不同的风格执行任务。

图 2 提供了系统的示意图。运动数据集 M 由一组参考运动组成，其中每个运动$m^{i}={\hat{q}_{t}^{i}}$表示为一系列姿态$\hat{q}_{t}^{i}$。运动片段可从真实演员的动作捕捉或艺术家创作的关键帧动画中收集。与先前的框架不同，我们的系统可直接应用于原始运动数据，无需任务特定标注或将片段分割为单个技能。模拟角色的运动由策略$\pi(a_{t} | s_{t}, ~g)$控制，该策略将角色状态$s_{t}$和给定目标 g 映射到动作 aₜ的分布。策略中的动作指定位于角色每个关节处的比例 - 微分（PD）控制器的目标位置，进而产生驱动角色运动的控制力。目标 g 指定任务奖励函数$r_{t}^{G}=r^{G}(~s_{t}, a_{t}, ~s_{t+1}, ~g)$，该函数定义了角色需满足的高层目标（例如，朝目标方向行走或对目标出拳）。风格目标$r_{t}^{S}=r^{S}(~s_{t}, ~s_{t+1})$由对抗性判别器指定，训练其区分数据集中描绘的运动与角色产生的运动。因此，风格目标充当与任务无关的运动先验，提供给定运动的自然度或风格的先验估计，与特定任务无关。然后，风格目标鼓励策略产生与数据集中描绘的行为相似的运动。

!\[系统示意图]\(Fig. 2 系统示意图说明文字)

图 2. 系统示意图。给定定义角色期望运动风格的运动数据集，系统训练运动先验，该先验在训练期间为策略指定风格奖励$r_{t}^{s}$。这些风格奖励与任务奖励$r_{t}^{G}$相结合，用于训练策略，使模拟角色满足特定任务目标 g，同时采用与数据集中参考运动相似的行为。

## 4 背景

我们的系统结合了目标条件强化学习和生成对抗性模仿学习技术，训练控制策略，使模拟角色能够以期望的行为风格完成具有挑战性的任务。在本节中，我们简要回顾这些技术。

### 4.1 目标条件强化学习

我们的角色通过目标条件强化学习框架进行训练，其中智能体根据策略 π 与环境交互，以实现根据目标分布$g ~ p(~g)$采样的给定目标$g \in G$。在每个时间步 t，智能体观察系统状态$s_{t} \in S$，然后从策略$a_{t} ~ \pi(a_{t} | s_{t}, ~g)$中采样动作$a_{t} \in A$。智能体然后执行该动作，产生新状态$S_{t+1}$以及标量奖励$r_{t}=r(~s_{t}, a_{t}, ~s_{t+1}, ~g)$。智能体的目标是学习一种策略，以最大化其期望折扣回报$J(\pi)$，其中$p(\tau | \pi, g)=p(~s_{0}) \prod_{t=0}^{T-1} p(~s_{t+1} | s_{t}, a_{t}) \pi(a_{t} | s_{t}, ~g)$表示策略 π 在目标 g 下轨迹$\tau={(s_{t}, a_{t}, r_{t})_{t=0}^{T-1}, ~s_{T}}$的可能性。$p(~s_{0})$是初始状态分布，$p(~s_{t+1} | s_{t}, a_{t})$表示环境动力学。T 表示轨迹的时间范围，$\gamma \in[0,1)$是折扣因子。

### 4.2 生成对抗性模仿学习

生成对抗性模仿学习（GAIL）\[Ho 和 Ermon，2016] 将为生成对抗网络（GAN）\[Goodfellow 等人，2014] 开发的技术应用于模仿学习领域。为简洁起见，我们在符号中省略目标 g，但以下讨论可轻松推广到目标条件设置。给定演示数据集$M=$ ${(s_{i}, a_{i})}$，包含从未知演示策略记录的状态和动作$a_{i}$，目标是训练策略$\pi(a | s)$来模仿演示者的行为。行为克隆可用于通过监督学习直接拟合策略，将 M 中观察到的状态映射到其相应的动作 \[Bojarski 等人，2016；Pomerleau，1988]。然而，如果仅提供少量演示，则行为克隆技术容易出现偏移 \[Ross 等人，2011]。此外，在演示动作不可观察的情况下（例如参考运动数据），行为克隆不直接适用。

GAIL 通过学习一种度量策略与演示之间相似性的目标函数，解决了行为克隆的部分局限性，然后通过强化学习更新策略以优化学习到的目标。该目标建模为判别器$D(~s, a)$，训练其预测给定状态 s 和动作 a 是从演示 M 中采样的，还是通过执行策略 π 生成的。$d^{M}(s, a)$和$d^{\pi}(s, a)$分别表示数据集和策略的状态 - 动作分布。然后，使用公式 1 中详细的 RL 目标训练策略，奖励由下式指定：

这种对抗性训练过程可解释为训练策略产生状态和动作，使判别器无法区分其与演示。可以证明，该目标最小化了$d^{M}(s, a)$和$d^{\pi}(s, a)$之间的詹森 - 香农散度 \[Ke 等人，2019；Nowozin 等人，2016]。

## 5 对抗性运动先验

在本文中，我们考虑由两个部分组成的奖励函数：1）角色应执行的任务；2）角色执行该任务的方式。

“任务内容” 由特定任务奖励$r^{G}(~s_{t}, a_{t}, ~s_{t}, ~g)$表示，定义了角色应满足的高层目标（例如，移动到目标位置）。“执行方式” 通过学习到的与任务无关的风格奖励$r^{s}(s_{t}, ~s_{t+1})$表示，指定了角色执行任务时应采用的行为低层细节（例如，走向目标还是跑向目标）。两个奖励项通过权重$w^{G}$和$w^{s}$线性组合。任务奖励$r^{G}$的设计相对直观简单。然而，设计能够使角色学习自然行为或符合特定风格行为的风格奖励$r^{S}$可能异常困难。因此，学习有效的风格目标将是本文的主要重点。

我们提议使用学习到的判别器对风格奖励进行建模，我们将其称为对抗性运动先验（AMP），类似于先前为基于视觉的姿态估计任务提出的对抗性姿态先验 \[Kanazawa 等人，2018]。与标准跟踪目标不同，标准跟踪目标测量相对于特定参考运动的姿态相似性，而运动先验返回一个通用分数，指示角色运动与数据集中描绘的运动的相似性，无需与特定运动片段显式比较。给定运动数据集，使用 GAIL 框架训练运动先验，以预测状态转移$(s_{t}, s_{t+1})$是来自数据集的真实样本，还是角色产生的伪造样本。运动先验与特定任务目标 g 无关，因此单个运动先验可应用于多个任务，不同的运动先验可用于训练以不同风格执行相同任务的策略。通过将 GAIL 与额外的任务目标相结合，我们的方法将任务指定与风格指定解耦，从而使我们的角色能够执行原始演示中未描绘的任务。然而，对抗性 RL 技术众所周知具有高度不稳定性。在以下部分中，我们讨论多项设计决策，以稳定训练过程并产生更高保真度的结果。

### 5.1 基于观察的模仿

GAIL 的原始公式需要访问演示者的动作 \[Ho 和 Ermon，2016]。然而，当演示以运动片段的形式提供时，演示者采取的动作是未知的，数据中仅观察到状态。为将 GAIL 扩展到仅含状态演示的场景，可在状态转移$D(~s, ~s')$上训练判别器，而非状态 - 动作对$D(~s, a)$\[Torabi 等人，2018]。$d^{M}(s, s')$和$d^{\pi}(s, s')$分别表示在数据集 M 中以及通过遵循策略 π 观察到从 s 到$s'$的状态转移的可能性。请注意，如果演示者与智能体不同（例如人类演员），则观察到的状态转移对于智能体可能在物理上不一致，因此智能体可能无法完美复现。尽管存在这种差异，我们表明判别器仍然为模仿各种行为提供了有效的目标。

### 5.2 最小二乘判别器

公式 5 中详细的标准 GAN 目标通常使用 sigmoid 交叉熵损失函数。然而，由于 sigmoid 函数饱和导致梯度消失，这种损失往往会导致优化挑战，可能阻碍策略的训练 \[Arjovsky 等人，2017]。已提出多种技术来解决这些优化挑战 \[Arjovsky 等人，2017；Berthelot 等人，2017；Gulrajani 等人，2017；Karras 等人，2017；Kodali 等人，2017；Mescheder 等人，2018；Radford 等人，2015；Salimans 等人，2016]。在本文中，我们采用为最小二乘 GAN（LSGAN）\[Mao 等人，2017] 提出的损失函数，该函数在图像合成任务中展示了更稳定的训练和更高质量的结果。使用以下目标训练判别器：

$\begin{array} {r}{\underset {D}{arg \operatorname* {min}} \mathbb {E}_{d^{M}(s,s^{\prime })}\left[ \left( D(s,s^{\prime })-1\right) ^{2}\right] +\mathbb {E}_{d^{\pi }(s,s^{\prime })}\left[ \left( D(s,s^{\prime })+1\right) ^{2}\right] .}\end{array}$

通过解决最小二乘回归问题训练判别器，为来自数据集的样本预测分数 1，为从策略记录的样本预测分数 - 1。然后，用于训练策略的奖励函数由下式给出：

如先前 RL 框架中的常见做法 \[Peng 等人，2018a，2016；Tassa 等人，2018]，应用额外的偏移、缩放和裁剪将奖励限制在 \[0,1] 之间。Mao 等人 \[2017] 表明，这种最小二乘目标最小化了$d^{M}(s, s')$和$d^{\pi}(s, s')$之间的皮尔逊$x^{2}$散度。

### 5.3 判别器观察

由于判别器为策略训练指定奖励，因此为判别器选择合适的特征集以进行预测，对于为策略提供有效反馈至关重要。因此，在将状态转移提供给判别器作为输入之前，我们首先应用观察映射$\Phi(s_{t})$，提取与确定给定运动特征相关的一组特征。然后，所得特征用作判别器$D(\Phi(s), \Phi(s'))$的输入。特征集包括：



* 根节点的线速度和角速度，在角色的局部坐标系中表示。

* 每个关节的局部旋转。

* 每个关节的局部速度。

* 末端执行器（例如手和脚）的 3D 位置，在角色的局部坐标系中表示。

根节点指定为角色的骨盆。角色的局部坐标系定义为原点位于根节点，x 轴沿根连杆的朝向，y 轴与全局上向量对齐。每个球关节的 3D 旋转使用对应于坐标系中法向量和切向量的两个 3D 向量进行编码。这种旋转编码提供了给定旋转的平滑且唯一的表示。为判别器选择的这组观察特征，旨在提供单个状态转移中运动的紧凑表示。观察结果也不包括任何特定任务特征，因此能够训练运动先验，而无需对参考运动进行任务特定标注，并允许使用相同数据集训练的运动先验用于不同任务。

### 5.4 梯度惩罚

GAN 中判别器和生成器之间的相互作用通常导致不稳定的训练动态。不稳定性的一个来源是判别器中的函数逼近误差，其中判别器可能在真实数据样本的流形上分配非零梯度 \[Mescheder 等人，2018]。这些非零梯度可能导致生成器过度偏移并偏离数据流形，而不是收敛到流形，导致训练期间的振荡和不稳定性。为缓解这种现象，可以应用梯度惩罚来惩罚数据集样本上的非零梯度 \[Gulrajani 等人，2017；Kodali 等人，2017；Mescheder 等人，2018]。我们纳入了这项技术以提高训练稳定性。然后，判别器目标由下式给出：


$ 
\arg\min_D \mathbb{E}_{d^M(s,s')} \left[ \left( D(\Phi(s), \Phi(s')) - 1 \right)^2 \right] + \mathbb{E}_{d^\pi(s,s')} \left[ \left( D(\Phi(s), \Phi(s')) + 1 \right)^2 \right] + \frac{w^{\text{gp}}}{2} \mathbb{E}_{d^{\text{mix}}(s,s')} \left[ \left\| \nabla_{\phi} D(\phi) \big|_{\phi=(\Phi(s), \Phi(s'))} \right\|^2 \right]
 $

其中$w^{\text{gp}}$是手动指定的系数。请注意，梯度惩罚是相对于观察特征$\phi=$ $(\Phi(s), \Phi(s'))$计算的，而非完整的状态特征集$(s, s')$。正如我们在实验中所示，梯度惩罚对于稳定训练和有效性能至关重要。

## 6 模型表示

给定高层任务目标和参考运动数据集，智能体负责学习一种控制策略，以实现任务目标，同时采用与数据集中描绘的运动相似的行为。在本节中，我们详细介绍学习框架各个组件的设计。

### 6.1 状态和动作

状态$s_{t}$由一组描述角色身体配置的特征组成。这些特征与 Peng 等人 \[2018a] 使用的特征相似，包括每个连杆相对于根节点的相对位置、使用 6D 法向 - 切向编码表示的每个连杆的旋转，以及连杆的线速度和角速度。所有特征都在角色的局部坐标系中记录。与先前的系统不同，先前的系统通过在状态中包含额外的相位信息（例如标量相位变量 \[Lee 等人，2019；Peng 等人，2018a] 或目标姿态 \[Bergamin 等人，2019；Chentanez 等人，2018；Won 等人，2020]）将策略与特定参考运动同步，我们的策略不训练为显式模仿数据集中的任何特定运动。因此，不需要此类同步或相位信息。

每个动作$a_{t}$指定位于角色每个关节处的 PD 控制器的目标位置。对于球关节，每个目标以 3D 指数映射$q \in \mathbb{R}^{3}$\[Grassia，1998] 的形式指定，其中旋转轴 v 和旋转角度 θ 可根据以下公式确定：

与先前系统中使用的 4D 轴角或四元数表示 \[Peng 等人，2018a；Won 等人，2020] 相比，这种表示提供了更紧凑的参数化，同时避免了欧拉角等参数化带来的万向节锁问题。旋转关节的目标旋转指定为 1D 旋转角度$q=\theta$。

### 6.2 网络架构

每个策略由神经网络建模，该神经网络将给定状态$s_{t}$和目标 g 映射到动作上的高斯分布$\pi(a_{t} | s_{t}, ~g)=$ $N(\mu(s_{t}, ~g), \sum )$，具有输入相关均值$\mu(s_{t}, ~g)$和固定对角协方差矩阵 Σ。均值由全连接网络指定，该网络具有两个隐藏层，包含 1024 和 512 个 ReLU 单元 \[Nair 和 Hinton，2010]， followed by a linear output layer. 协方差矩阵$\sum =diag(\sigma_{1}, \sigma_{2}, ...)$的值是手动指定的，并在训练过程中保持固定。价值函数和判别器$D(~s_{t}, ~s_{t+1})$由单独的网络建模，其架构与策略相似。

### 6.3 训练

我们的策略使用 GAIL \[Ho 和 Ermon，2016] 和近端策略优化（PPO）\[Schulman 等人，2017] 的组合进行训练。算法 1 提供了训练过程的概述。在每个时间步 t，智能体从环境接收任务奖励$r_{t}^{G}=r^{G}(~s_{t}, a_{t}, ~s_{t+1}, ~g)$，然后查询运动先验以获取风格奖励$r_{t}^{S}=r^{S}(~s_{t}, ~s_{t+1})$，根据公式 7 计算。根据公式 4 将两个奖励组合，得到特定时间步的奖励。遵循 Peng 等人 \[2018a] 提出的方法，我们纳入了参考状态初始化和早期终止机制。参考状态初始化通过将角色初始化为从数据集中所有运动片段中随机采样的状态来实现。对于大多数任务，当角色身体的任何部分（脚除外）与地面接触时，触发早期终止。对于接触更丰富的任务（例如翻滚或摔倒后站起），禁用此终止标准。

一旦使用策略收集了一批数据，记录的轨迹用于更新策略和价值函数。使用$TD(\lambda)$\[Sutton 和 Barto，1998] 计算的目标值更新价值函数。使用 GAE (𝜆)\[Schulman 等人，2015] 计算的优势更新策略。从策略记录的每个轨迹也存储在回放缓冲区 B 中，包含来自过去训练迭代的轨迹。根据公式 8，使用从参考运动数据集 M 采样的转换$(s, s')$小批量和来自回放缓冲区 B 的转换，更新判别器。回放缓冲区通过防止判别器过度拟合到来自策略的最新轨迹批次，帮助稳定训练。

## 7 任务

为评估 AMP 控制角色运动风格的有效性，我们将我们的框架应用于训练复杂的 3D 模拟角色，以不同的运动风格执行各种运动控制任务。角色包括 34 自由度的人形角色、59 自由度的霸王龙和 64 自由度的狗。每个任务的摘要如下。有关每个任务及其各自奖励函数的更详细描述，请参见附录 A。

### 目标朝向

在此任务中，角色的目标是沿目标朝向方向$d^{*}$以目标速度$v^{*}$移动。策略的目标指定为$g_{t}=(\tilde{d}_{t}^{*}, v^{*})$，其中$\overline{d}_{t}^{*}$是角色局部坐标系中的目标方向。目标速度在$v^{*} \in[1,5] m / s$之间随机选择。对于较慢的运动风格（如僵尸和潜行风格），目标速度固定为 1m/s。

### 目标位置

在此任务中，角色的目标是移动到目标位置$x^{*}$。目标$g_{t}=\overline{x}_{t}^{*}$记录角色局部坐标系中的目标位置。

### 运球

为在更复杂的物体操作任务上评估我们的系统，我们训练运球任务的策略，其中角色的目标是将足球运送到目标位置。目标$g_{t}=\overline{x}_{t}^{*}$记录目标位置相对于角色的相对位置。状态$s_{t}$增加了描述球状态的额外特征，包括球在角色局部坐标系中的位置$\overline{x}_{t}^{ball }$、方向$\overline{q}_{t}^{ball }$、线速度$\overline{\dot{x}}_{t}^{ball }$和角速度$\overline{\dot{q}}_{t}^{ball }$。

### 击打

为展示 AMP 组合多种行为的能力，我们考虑角色目标是使用指定的末端执行器（例如手）击打目标的任务。目标可能位于距离角色不同的位置。因此，角色必须首先靠近目标，然后再击打它。这些不同的阶段需要不同的最优行为，因此需要策略组合并在适当的技能之间转换。目标$g_{t}=(\overline{x}_{t}^{*}, h_{t})$记录角色局部坐标系中目标$\bar{x}_{t}^{*}$的位置，以及指定目标是否已被击中的指示变量$h_{t}$。

### 障碍

最后，我们考虑涉及视觉感知和与更复杂环境交互的任务，其中角色的目标是穿越充满障碍的地形，同时保持目标速度。为两种类型的环境训练策略：1）包含间隙、台阶和需要角色低头通过的头顶障碍物组合的环境；2）包含需要更精确接触规划的狭窄踏脚石的环境。环境示例见图 1 和图 3。为使策略能够感知即将到来的障碍物，状态增加了即将到来的地形的 1D 高度场。

## 8 结果

我们在一系列具有挑战性的运动控制任务上，使用复杂的模拟角色评估了我们框架的有效性。首先，我们表明我们的方法可以轻松扩展到包含多样化运动片段的大型非结构化数据集，从而使我们的角色能够通过模仿数据集中的行为，以自然逼真的方式完成具有挑战性的任务。角色自动学习从运动数据中组合和泛化不同的技能，以实现高层任务目标，无需显式运动选择机制。然后，我们在单片段模仿任务上评估 AMP，并表明我们的方法能够紧密模仿多种动态和杂技技能，生成与人类演员记录的参考运动几乎无法区分的运动。角色学习的行为可在补充视频中查看。

### 8.1 实验设置

所有环境均使用 Bullet 物理引擎 \[Coumans 等人，2013] 进行模拟，模拟频率为 1.2kHz。策略的查询频率为 30Hz，每个动作指定角色关节处 PD 控制器的目标位置。所有神经网络均使用 Tensorflow \[Abadi 等人，2015] 实现。梯度惩罚系数设置为$w^{gp}=10$。详细的超参数设置见附录 B。参考运动片段来自公共动作捕捉库、自定义录制的动作捕捉片段和艺术家创作的关键帧动画的组合 \[CMU \[未注明日期]；SFU \[未注明日期]；Zhang 等人，2018]。根据任务和角色的不同，每个策略使用 100-300 百万个样本进行训练，在 16 个 CPU 内核上大约需要 30-140 小时。我们系统的代码将在本文发表后发布。

### 8.2 任务

在本节中，我们展示了 AMP 在角色执行其他高层任务时控制其运动风格的有效性。所有任务的任务奖励和风格奖励权重均设置为$w^{G}=0.5$和$w^{s}=0.5$。通过为运动先验提供不同的数据集，可以训练角色以多种不同的风格执行任务。图 3 展示了人形角色在各种任务上学习到的行为。表 1 记录了策略相对于标准化任务回报的性能，用于训练运动先验的不同数据集的汇总统计数据见表 2。AMP 可以容纳大型非结构化数据集，最大的数据集包含来自 8 个不同人类演员的 56 个片段，总运动数据时长为 434 秒。在目标朝向任务中，使用包含行走、跑步和慢跑运动的移动数据集训练的运动先验，导致策略根据目标速度执行不同的移动步态。

!\[人形角色在各任务上的行为]\(Fig. 3 人形角色行为说明文字)

图 3. 运动先验可以使用大型多样化运动数据集进行训练，使模拟角色能够通过组合更广泛的技能来完成复杂任务。每个环境标注为 “角色：任务（数据集）”。

表 1. AMP 与额外任务目标组合的性能统计。性能记录为平均标准化任务回报，其中 0 是每集的最小可能回报，1 是最大可能回报。回报在 3 个不同随机种子初始化的模型上平均，每个模型记录 32 集。运动先验可以使用不同的数据集进行训练，以产生在执行特定任务时采用不同风格行为的策略。



| 角色  | 任务   | 数据集          | 任务回报      |
| --- | ---- | ------------ | --------- |
| 人形  | 目标朝向 | 移动           | 0.90±0.01 |
|     |      | 行走           | 0.46±0.01 |
|     |      | 跑步           | 0.63±0.01 |
|     |      | 潜行           | 0.89±0.02 |
|     |      | 僵尸           | 0.94±0.00 |
|     | 目标位置 | 移动           | 0.63±0.01 |
|     |      | 僵尸           | 0.50±0.00 |
|     | 障碍   | 跑步 + 跳跃 + 翻滚 | 0.27±0.10 |
|     | 踏脚石  | 侧手翻          | 0.43±0.03 |
|     |      | 跳跃           | 0.56±0.12 |
|     | 运球   | 移动           | 0.78±0.05 |
|     |      | 僵尸           | 0.60±0.04 |
|     | 击打   | 行走 + 出拳      | 0.73±0.02 |
| 霸王龙 | 目标位置 | 移动           | 0.36±0.03 |

各种步态之间的转换通过运动先验自动涌现，角色在慢速（约 1m/s）时采用行走步态，在较快速度（约 2.5m/s）时切换到慢跑步态，当目标速度接近（约 4.5m/s）时开始快速奔跑。运动先验还导致其他类人策略，例如转弯时倾斜身体，以及在方向发生大变化前减速。策略为目标位置任务开发了类似的行为。当目标靠近角色时，策略采用较慢的行走步态。但当目标更远时，角色自动转换为跑步。这些复杂的行为从运动先验中自然产生，无需运动规划器显式选择角色在特定场景下应执行的运动，例如先前系统中使用的运动规划器 \[Bergamin 等人，2019；Luo 等人，2020；Peng 等人，2017]。除了标准的移动步态外，运动先验还可以训练用于更多风格化的行为，例如像蹒跚的僵尸一样行走或潜行。我们的框架通过简单地为运动先验提供不同的非结构化运动数据集，使角色能够获得这些不同的风格。

为确定不同步态之间的转换是运动先验的产物还是任务目标的结果，我们训练策略使用仅包含行走或跑步数据的有限数据集执行目标朝向任务。图 4 比较了使用这些不同数据集训练的策略的性能。仅使用行走运动训练的策略仅学习执行行走步态，即使在更快的目标速度下也不会转换到更快的跑步步态。因此，这些策略无法达到更快的目标速度。同样，仅使用跑步运动训练的策略无法匹配较慢的目标速度。使用多样化数据集训练运动先验，可产生更灵活和最优的策略，能够实现更广泛的目标速度。这表明我们的策略所展现的行为多样性在很大程度上可归因于运动先验，而不仅仅是任务目标的结果。

表 2. 用于训练运动先验的不同数据集的汇总统计。我们记录每个数据集运动片段的总长度，以及片段数量和录制片段的受试者（例如人类演员）数量。



| 角色  | 数据集          | 时长（秒） | 片段数 | 受试者数 |
| --- | ------------ | ----- | --- | ---- |
| 人形  | 侧手翻          | 13.6  | 3   | 1    |
|     | 跳跃           | 28.6  | 10  | 4    |
|     | 移动           | 434.1 | 56  | 8    |
|     | 跑步           | 204.4 | 47  | 3    |
|     | 跑步 + 跳跃 + 翻滚 | 22.1  | 10  | 7    |
|     | 潜行           | 136.5 | 3   | 1    |
|     | 行走           | 229.6 | 9   | 5    |
|     | 行走 + 出拳      | 247.8 | 15  | 9    |
|     | 僵尸           | 18.3  | 1   | 1    |
| 霸王龙 | 移动           | 10.5  | 5   | 1    |

为进一步说明 AMP 组合不同技能的能力，我们将额外的参考运动引入数据集中，包括从地面以各种配置站起的运动。这些额外的运动片段使我们的角色能够从摔倒中恢复并继续执行给定任务（图 3 (c)）。策略还发现了数据集中不存在的新颖恢复行为。当角色向前摔倒时，它会在摔倒过程中蜷缩身体翻滚，以便更快地转换到站起行为。虽然这种特定行为在运动片段中不存在，但策略能够泛化数据集中观察到的行为，为新场景产生新颖且自然的策略。

!\[目标朝向任务策略性能比较]\(Fig. 4 目标朝向任务策略性能比较说明文字)

图 4. 使用不同数据集训练的目标朝向策略的性能。左图：学习曲线比较使用大型多样化移动片段数据集训练的策略与仅使用行走或跑步参考运动训练的策略的标准化任务回报。每个数据集训练 3 个模型。右图：比较目标速度与不同策略实现的平均速度。使用更大的移动数据集训练的策略能够通过模仿不同的步态，更紧密地跟随各种目标速度。

对于击打任务（图 1），使用行走运动片段和出拳运动片段的集合训练运动先验。所得策略学习在目标较远时走向目标，然后在范围内击打目标时转换为出拳运动。请注意，数据集中的运动片段仅包含纯行走运动或纯出拳运动，没有片段显示演员走向目标并对其出拳。相反，策略学习在时间上对这些不同的行为进行排序，以实现高层任务目标。同样，不同技能的组合从运动先验中自动涌现，无需运动规划器或其他运动选择机制。

最后，我们的系统还可以训练用于穿越障碍环境的视觉运动策略。通过为运动先验提供移动片段和翻滚片段的集合，角色学习利用这些多样化的行为来穿越不同的障碍。角色学习跳过诸如间隙之类的障碍。但当接近头顶障碍物时，角色转换为翻滚行为以从障碍物下方通过。先前展示了类似的多样化动作组合以清除障碍的系统，通常需要单独的运动规划器或手动标注 \[Liu 等人，2012；Park 等人，2019]。我们的方法提供了一个统一的框架，其中相同的基础算法能够学习如何执行各种技能以及在特定场景下执行哪种技能。此外，通过为运动先验提供不同的运动片段（例如在踏脚石上跳跃或侧手翻），还可以训练角色以不同的风格穿越障碍（图 3）。

!\[单片段模仿任务中人形角色学习的行为]\(Fig. 6 单片段模仿任务行为说明文字)

图 6. 人形角色在单片段模仿任务上学习的行为快照。从上到下：后空翻、侧空翻、侧手翻、旋转、回旋踢、翻滚。AMP 使角色能够紧密模仿多种高度动态和杂技技能。

### 8.3 比较

从非结构化运动数据中学习运动先验的另一种方法是构建 latent 空间模型 \[Heess 等人，2016；Lynch 等人，2020；Merel 等人，2020；Peng 等人，2019a]。与 AMP 不同，AMP 通过优化目标直接鼓励角色采用期望的运动风格，而 latent 空间模型通过使用 latent 表示将策略的动作约束为产生期望风格的运动，间接强制特定的运动风格。为了将 AMP 与这些 latent 空间模型进行比较，我们首先使用运动跟踪目标预训练低层控制器，以模仿用于训练运动先验的相同参考运动集合。然后，使用学习到的低层控制器为每个下游任务训练单独的高层控制器。请注意，参考运动仅在预训练期间使用，高层控制器训练为仅优化任务目标。有关 latent 空间模型实验设置的更详细描述，请参见附录 C。

AMP 和 latent 空间模型学习的行为的定性比较见补充视频。图 5 比较了不同模型的任务性能，以及每个任务从零开始训练且不利用任何运动数据的基线模型。AMP 和 latent 空间模型都能够产生比基线模型明显更逼真的行为。对于 latent 空间模型，由于低层和高层控制器是分开训练的，高层控制器指定的编码分布可能与低层控制器在预训练期间观察到的编码分布不同 \[Luo 等人，2020]。这进而可能导致偏离原始数据集中描绘的行为的不自然运动。AMP 通过奖励函数直接强制运动风格，因此能够更好地缓解其中一些伪影。来自 latent 空间模型的更结构化的探索行为使策略能够更快地解决下游任务。然而，用于构建低层控制器的预训练阶段本身可能需要大量样本。在我们的实验中，低层控制器在转移到下游任务之前，使用 300 百万个样本进行训练。使用 AMP，不需要此类预训练，运动先验可以与策略联合训练。

### 8.4 单片段模仿

尽管我们的目标是使用大型运动数据集训练角色，但为了评估我们的框架从运动片段中模仿行为的有效性，我们考虑单片段模仿任务。在这种情况下，角色的目标是一次模仿单个运动片段，没有额外的任务目标。因此，策略仅训练为最大化来自运动先验的风格奖励。与先前的运动跟踪方法不同，我们的方法不需要手动设计的跟踪目标或参考运动与策略之间的基于相位的同步 \[Peng 等人，2018a]。表 3 总结了使用 AMP 训练的策略模仿多种运动的性能。图 6 和图 7 展示了角色学习的运动示例。使用平均姿态误差评估性能，其中每个时间步 t 的姿态误差$e_{t}^{pose }$是根据每个关节相对于根节点的相对位置（以米为单位），计算模拟角色姿态与参考运动之间的误差。$x_{t}^{j}$和$\hat{x}_{t}^{j}$分别表示模拟角色和参考运动中关节 j 的 3D 笛卡尔位置，$N^{joint }$是角色身体中的关节总数。先前有报道称，这种评估运动相似性的方法更符合人类对运动相似性的感知 \[Harada 等人，2004；Tang 等人，2008]。由于 AMP 不使用相位变量来同步策略与参考运动，运动可能以不同的速度进行，导致即使整体运动相似，也可能出现大的姿态误差。为了更好地评估运动的相似性，我们首先应用动态时间规整（DTW）来对齐参考运动与模拟角色的运动 \[Sakoe 和 Chiba，1978]，然后计算两个对齐运动之间的姿态误差。使用公式 10 作为成本函数应用 DTW。

AMP 能够紧密模仿多种高度动态的技能，同时避免了先前对抗性运动模仿系统所展现的许多视觉伪影 \[Merel 等人，2017；Wang 等人，2017]。我们将我们系统的性能与 Peng 等人 \[2018a] 的运动跟踪方法产生的结果进行比较，该方法使用手动设计的奖励函数，并需要通过相位变量将策略与参考运动同步。图 8 比较了不同方法的学习曲线。由于基于跟踪的策略与各自的参考运动同步，它们通常能够更快地学习并实现比使用 AMP 训练的策略更低的误差。尽管如此，我们的方法能够产生相当质量的结果，而无需为不同的运动手动设计或调整奖励函数。然而，对于某些运动（例如前空翻），AMP 容易收敛到局部最优行为，角色不是执行空翻，而是学习简单地向前移动以避免摔倒。基于跟踪的方法可以通过如果角色的姿态与参考运动偏离太远，则提前终止 episode 来缓解这些局部最优解 \[Peng 等人，2018a；Won 等人，2020]。然而，这种策略直接应用于 AMP，因为策略与参考运动不同步。但如前几节所示，正是这种缺乏同步使得 AMP 能够轻松利用大型多样化运动数据集来解决更复杂的任务。

!\[非人形角色学习的运动]\(Fig. 7 非人形角色运动说明文字)

图 7. AMP 可用于训练复杂的非人形角色，例如 59 自由度的霸王龙和 64 自由度的狗。通过为运动先验提供不同的参考运动片段，可以训练角色执行各种移动步态，例如小跑和慢跑。

表 3. 在没有任务目标的情况下模仿单个运动片段的性能统计。“数据集大小” 记录用于每个技能的运动数据的总长度。性能记录为参考运动和模拟角色的时间规整轨迹之间的平均姿态误差（以米为单位）。姿态误差在 3 个不同随机种子初始化的模型上平均，每个模型记录 32 集。每集的最大长度为 20 秒。我们将我们的方法（AMP）与 Peng 等人 \[2018a] 提出的运动跟踪方法进行比较。AMP 能够紧密模仿多种复杂运动，无需手动奖励设计。



| 角色  | 运动  | 数据集大小  | 运动跟踪        | AMP（本文方法）   |
| --- | --- | ------ | ----------- | ----------- |
| 人形  | 后空翻 | 1.75 秒 | 0.076±0.021 | 0.150±0.028 |
|     | 侧手翻 | 2.72 秒 | 0.039±0.011 | 0.067±0.014 |
|     | 爬行  | 2.93 秒 | 0.044±0.001 | 0.049±0.007 |
|     | 跳舞  | 1.62 秒 | 0.038±0.001 | 0.055±0.015 |
|     | 前空翻 | 1.65 秒 | 0.278±0.040 | 0.425±0.010 |
|     | 慢跑  | 0.83 秒 | 0.029±0.001 | 0.056±0.001 |
|     | 跳跃  | 1.77 秒 | 0.033±0.001 | 0.083±0.022 |
|     | 翻滚  | 2.02 秒 | 0.072±0.018 | 0.088±0.008 |
|     | 跑步  | 0.80 秒 | 0.028±0.002 | 0.075±0.015 |
|     | 旋转  | 1.00 秒 | 0.063±0.022 | 0.047±0.002 |
|     | 侧空翻 | 2.44 秒 | 0.191±0.043 | 0.124±0.012 |
|     | 回旋踢 | 1.28 秒 | 0.042±0.001 | 0.058±0.012 |
|     | 行走  | 1.30 秒 | 0.018±0.005 | 0.030±0.001 |
|     | 僵尸  | 1.68 秒 | 0.049±0.013 | 0.058±0.014 |
| 霸王龙 | 转弯  | 2.13 秒 | 0.098±0.011 | 0.284±0.023 |
|     | 行走  | 2.00 秒 | 0.069±0.005 | 0.096±0.027 |
| 狗   | 慢跑  | 0.45 秒 | 0.026±0.002 | 0.034±0.002 |
|     | 踱步  | 0.63 秒 | 0.020±0.001 | 0.024±0.003 |
|     | 旋转  | 0.73 秒 | 0.026±0.002 | 0.086±0.008 |
|     | 小跑  | 0.52 秒 | 0.019±0.001 | 0.026±0.001 |

### 8.5 消融实验

我们的系统能够产生比先前用于基于物理角色控制的对抗性学习框架更高保真度的运动 \[Merel 等人，2017；Wang 等人，2017]。在本节中，我们确定了导致更稳定训练和更高质量结果的关键设计决策。图 8 比较了在单片段模仿任务上训练的策略的学习曲线，其中系统的不同组件被禁用。梯度惩罚被证明是最重要的组件。没有这种正则化训练的模型在训练过程中往往表现出较大的性能波动，并在最终策略中导致明显的视觉伪影，如补充视频所示。添加梯度惩罚不仅提高了训练期间的稳定性，还导致在大量技能上的学习速度显著加快。判别器观察中包含速度特征，也是模仿某些运动的重要组件。原则上，将连续姿态作为输入提供给判别器应该提供一些可用于推断关节速度的信息。但我们发现，这对于某些运动（例如翻滚）是不够的。如补充视频所示，在没有速度特征的情况下，角色容易收敛到在地面上保持固定姿态的策略，而不是执行翻滚。额外的速度特征能够缓解这些不期望的行为。

!\[单片段模仿任务不同方法的学习曲线]\(Fig. 8 单片段模仿任务学习曲线说明文字)

图 8. 单片段模仿任务上各种方法的学习曲线。我们将 AMP 与 Peng 等人 \[2018a] 提出的运动跟踪方法（运动跟踪）、没有判别器速度特征的 AMP 版本（AMP - 无速度）以及没有梯度惩罚正则化的 AMP（AMP - 无 GP）进行比较。所有技能的综合学习曲线见附录。与先前的基于跟踪的方法相比，AMP 产生了相当质量的结果，无需手动设计的奖励函数或策略与参考运动之间的同步。速度特征和梯度惩罚对于在具有挑战性的技能上获得有效且一致的结果至关重要。

## 9 讨论和局限性

在本文中，我们提出了一种用于基于物理角色动画的对抗性学习系统，该系统能够让角色从大型非结构化数据集中模仿多种行为，无需运动规划器或其他片段选择机制。我们的系统允许用户指定控制角色行为的高层任务目标，而角色运动的更精细低层风格可通过学习到的运动先验进行控制。为实现任务目标而进行的不同技能组合，会从运动先验中自动涌现。运动先验还使我们的角色能够紧密模仿多种高度动态的技能，并产生与基于跟踪的技术相当的结果，无需手动奖励设计或控制器与参考运动之间的同步。

我们的系统表明，对抗性模仿学习技术确实能够为复杂技能生成高保真度的运动。然而，与许多其他基于 GAN 的技术一样，AMP 容易出现模式崩溃。当提供大型多样化运动片段数据集时，策略倾向于仅模仿示例行为的一小部分，而忽略可能最终对特定任务更优的其他行为。我们实验中的运动先验也为每个策略从零开始训练。但由于运动先验在很大程度上与任务无关，原则上应该能够将运动先验转移和重用于不同的策略和任务。探索开发通用和可转移运动先验的技术，可能会产生模块化的目标函数，这些函数可以方便地纳入下游任务，而无需为每个新任务重新训练。尽管运动先验不需要直接访问特定任务信息，但用于训练运动先验的数据是由为执行特定任务训练的策略生成的。这可能会在运动先验中引入一些任务依赖性，从而阻碍其转移到其他任务的能力。使用从更大和更多样化的任务集合生成的数据训练运动先验，可能有助于促进将学习到的运动先验转移到新任务。我们的实验也主要集中在涉及技能时间组合的任务上，这些任务要求角色在不同时间执行不同的行为，以实现特定的任务目标，例如走向目标然后出拳。然而，空间组合对于某些需要角色同时执行多种技能的任务也可能至关重要。开发更适合不同技能空间组合的运动先验，可能会产生更灵活和复杂的行为。尽管存在这些局限性，我们希望这项工作提供了一个有用的工具，使基于物理模拟的角色能够利用在运动学动画技术中如此有效的大型运动数据集，并为数据驱动的基于物理角色动画的未来探索开辟令人兴奋的方向。

## 致谢

我们感谢索尼互动娱乐为该项目提供参考运动数据，Bonny Ho 为视频配音，匿名评审人员的有益反馈，以及 AWS 提供的计算资源。本研究得到了 NSERC 研究生奖学金和伯克利研究生学习奖学金的资助。

（参考文献部分省略）

## 附录

### A 任务

在本节中，我们提供每个任务的详细描述，以及训练期间使用的任务奖励函数。

#### 目标朝向

在此任务中，角色的目标是沿目标朝向方向$d^{*}$以目标速度$v^{*}$移动。策略的目标输入指定为$g_{t}=(\tilde{d}_{t}^{*}, v^{*})$，其中$\overline{d}_{t}^{*}$是角色局部坐标系中的目标方向。任务奖励计算如下：其中$\dot{x}_{t}^{com}$是时间步 t 角色的质心速度，目标速度在$u^{*} \in$ \[1,5] m/s 之间随机选择。对于较慢的运动风格（如僵尸和潜行风格），目标速度固定为 1m/s。

#### 目标位置

在此任务中，角色的目标是移动到目标位置$x^{*}$。目标$g_{t}=\overline{x}_{t}^{*}$记录角色局部坐标系中的目标位置。任务奖励由下式给出：

这里，指定了角色朝向目标移动的最小目标速度，角色不会因移动速度超过此阈值而受到惩罚。$d_{t}^{*}$是在水平面上从角色根节点指向目标的单位向量。

$r_{t}^{G}=exp \left(-0.25\left(v^{*}-d^{*} \cdot \dot{x}_{t}^{com }\right)^{2}\right), \quad(11)$

#### 运球

为在更复杂的物体操作任务上评估我们的系统，我们训练运球任务的策略，其中目标是让角色将足球运送到目标位置。奖励函数由下式给出：$r_{t}^{c v}$和$r_{t}^{c p}$鼓励角色走向球并保持在球附近，其中$x_{t}^{ball }$和$\dot{x}_{t}^{ball }$表示球的位置和速度，$d_{t}^{ball }$是从角色指向球的单位向量，$v^{*}=1 ~m / s$是角色朝向球移动的目标速度。类似地，$r_{t}^{bv}$和$r_{t}^{bp}$鼓励角色将球移动到目标位置，$d_{t}^{*}$表示从球指向目标的单位向量。目标$g_{t}=\overline{x}_{t}^{*}$记录目标位置相对于角色的相对位置。状态$s_{t}$增加了描述球状态的额外特征，包括球在角色局部坐标系中的位置$\overline{x}_{t}^{ball }$、方向$\overline{q}_{t}^{ball }$、线速度$\overline{x}_{t}^{ball }$和角速度$\overline{\dot{q}}_{t}^{ball }$。

#### 击打

最后，为进一步展示我们方法组合多种行为的能力，我们考虑角色目标是使用指定的末端执行器（例如手）击打目标的任务。目标可能位于距离角色不同的位置。因此，角色必须首先靠近目标，然后再击打它。任务的这些不同阶段需要不同的最优行为，因此需要策略组合并在适当的技能之间转换。目标$g_{t}=(\overline{x}_{t}^{*}, h_{t})$记录角色局部坐标系中目标$\overline{x}_{t}^{*}$的位置，以及指定目标是否已被击中的指示变量$h_{t}$。任务奖励分为三个阶段：

$r_{t}^{G}= \begin{cases}1, & ç®æ å·²è¢«å»ä¸­ \\ 0.3 r_{t}^{near }+0.3, & \left\| x^{*}-x_{t}^{root }\right\| <1.375 m . & (18) \\ 0.3 r_{t}^{far }, & å¶ä»æåµ \end{cases}$

如果角色远离目标$x^{*}$，$r_{t}^{far}$鼓励角色使用与目标位置任务类似的奖励函数（公式 12）移动到目标。一旦角色在目标的给定距离内，$r_{t}^{near }$鼓励角色使用特定的末端执行器击打目标，其中$x_{t}^{eff }$和$\dot{x}_{t}^{eff}$表示末端执行器的位置和速度，$d_{t}^{*}$是从角色根节点指向目标的单位向量。击打目标后，角色在剩余时间步中获得恒定奖励 1。

#### 障碍

最后，我们考虑涉及视觉感知和与更复杂环境交互的任务，其中角色的目标是穿越充满障碍的环境，同时保持目标速度。为两种类型的环境训练策略：1）包含间隙、台阶和需要角色低头通过的头顶障碍物组合的环境；2）包含需要更精确接触规划的狭窄踏脚石的环境。环境示例见图 1 和图 3。任务奖励与目标朝向任务使用的奖励相同（公式 11），但目标朝向固定为前进方向。为使策略能够感知即将到来的障碍物，状态增加了即将到来的地形的 1D 高度场。高度场记录了角色前方 10 米内均匀分布的 100 个采样位置的地形高度。

### B AMP 超参数

AMP 实验中使用的超参数设置见表 4。对于单片段模仿任务，我们发现较小的折扣因子$\gamma=0.95$允许角色更紧密地模仿给定的参考运动。对于包含额外任务目标的实验，使用较大的折扣因子$\gamma=0.99$，因为这些任务可能需要更长的范围规划，例如运球和击打。

表 4. AMP 超参数。



| 参数                | 数值     |
| ----------------- | ------ |
| 任务奖励权重 $w^{G}$    | 0.5    |
| 风格奖励权重 $w^{S}$    | 0.5    |
| 梯度惩罚 $wgp$        | 10     |
| 每次更新迭代的样本数        | 4096   |
| 批次大小              | 256    |
| 判别器批次大小 $K$       | 256    |
| 策略步长（单片段模仿）$\pi$  | 2×10⁻⁶ |
| 策略步长（任务）$\pi$     | 4×10⁻⁶ |
| 价值步长（单片段模仿）$V$    | 10⁻⁴   |
| 价值步长（任务）$V$       | 2×10⁻⁵ |
| 判别器步长 $D$         | 10⁻⁵   |
| 判别器回放缓冲区大小 $B$    | 10⁵    |
| 折扣（单片段模仿）$\gamma$ | 0.95   |
| 折扣（任务）$\gamma$    | 0.99   |
| SGD 动量            | 0.9    |
| GAE(𝜆)           | 0.95   |
| TD(𝜆)            | 0.95   |
| PPO 裁剪阈值          | 0.2    |

### C  latent 空间模型

latent 空间模型遵循与 Peng 等人 \[2019a] 和 Merel 等人 \[2019] 类似的架构。在预训练期间，编码器$q(z_{t} | g_{t})$将目标映射到 latent 变量$z_{t}$的分布。然后从编码器分布中采样 latent 编码$z_{t} ~ q(z_{t} | g_{t})$，并将其作为输入传递给策略$\pi(a_{t} | s_{t}, z_{t})$。 latent 分布建模为高斯分布$q(z_{t} | g_{t})=$ $N(\mu_{q}(~g_{t}), \sum _{q}(~g_{t}))$，具有均值$\mu_{q}(~g_{t})$和对角协方差矩阵$\sum _{q}(~g_{t})$。使用以下目标联合训练编码器和策略：表示目标增强轨迹，其中目标$g_{t}$可能在每个时间步变化，是给定策略 π 和编码器 q 下轨迹的可能性。类似于 VAE，我们包含相对于变分先验$p_{0}(z_{t})=N(0, I)$和系数 λ 的 KL 正则化器。使用重参数化技巧 \[Kingma 和 Welling，2014]，通过 PPO 端到端训练策略和编码器。训练完成后，可通过将$\pi(a_{t} | s_{t}, z_{t})$用作低层控制器，并训练单独的高层控制器$u(z_{t} | s_{t}, ~g_{t})$为低层控制器指定 latent 编码$z_{t}$，将 latent 空间模型转移到下游任务。π 的参数固定，为每个下游任务训练新的高层控制器 u。

$r_{t}^{G}=0.5 r_{t}^{heading }+0.5 r_{t}^{waving }, (22)$

在预训练期间，使用运动模仿训练 latent 空间模型，目标是让角色模仿一组运动片段。在每个 episode 开始时随机选择参考运动，每 5-10 秒选择新的参考运动。目标指定未来两个时间步参考运动的目标姿态。

用于 π 和 u 的网络遵循与使用 AMP 训练的策略所用网络类似的架构。编码器 q 由包含两个隐藏层的网络建模，具有 512 和 256 个隐藏单元， followed by a linear output layer for $\mu_{q}(~g_{t})$ and $\sum _{q}(~g_{t})$。 latent 编码的大小设置为 16 维。

> （注：文档部分内容可能由 AI 生成）